{"nbformat":4,"nbformat_minor":0,"metadata":{"celltoolbar":"Slideshow","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"week-5-inclass-non-tabular-scrape_BLANKS.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"s1gmCxq4kQCP"},"source":["# Scraping non-tabular data\n","\n","Look at the first section of CEO data on <a href=\"https://sandeepmj.github.io/scrape-example-page/#organized\">this page</a>:\n","\n","```https://sandeepmj.github.io/scrape-example-page/#organized```"]},{"cell_type":"markdown","metadata":{"id":"YfF9k_35kQCS"},"source":["### This won't be as simple as scraping a table:\n","\n","* Where and how is the content held on the page?\n","* How can we access it?\n","* Is there a pattern?\n","* Is there anything that breaks the pattern?"]},{"cell_type":"code","metadata":{"id":"IoRTykDqkQCS"},"source":["## import libraries\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import requests ## a library that returns information from websites."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gu9tCcmGkQCT"},"source":["# url to scrape\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5rwRWytdkQCT"},"source":["## We use the request library to grab page content\n","## get contents of URL and store in object called page.\n","## Let's print page and see what we have.\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0E4AW8fbkQCU"},"source":["## what type of object is it?\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6spaTdiWkQCU"},"source":["## We'll used BeautifulSoup to convert content into content we recognize\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j8vbDKpokQCU"},"source":["## prettify the print so it's easier to see the HTML tree."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v1179X78kQCV"},"source":["## Non-tabular data in classes"]},{"cell_type":"code","metadata":{"id":"LPqaD4GZkQCV"},"source":["## soup captures the entire page.\n","## we narrow it down to capture our target section and store in object called dis\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R9H6P0BLkQCV"},"source":["## find class ceo and store in object called ceos\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7bseM95nkQCW"},"source":["### Decompose"]},{"cell_type":"code","metadata":{"id":"NWVfsdWJkQCW"},"source":["### remove spans\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"getdCBY6kQCW"},"source":["## get ranks into a list\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hHvQiwI4kQCW"},"source":["## get names into a list\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-bcW23CskQCX"},"source":["## annual compensation\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LR30fJZzkQCX"},"source":["## company name\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GtiX9CcgkQCX"},"source":["## Pandas to create data frame\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"071z2sBGkQCX"},"source":["## What if there are dozens or more column header names?\n","### That's a lot of typing"]},{"cell_type":"code","metadata":{"id":"AEbafU5wkQCX"},"source":["## let's quickly recreate our soup.\n","## in the future we'll grab the labels in our early steps\n","soup = BeautifulSoup(page.content, \"html.parser\")\n","organized = soup.find(\"section\", id=\"organized\")\n","organized"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x5dWnbL0kQCY"},"source":["## find the labels (column headers)\n","## using list comprehension save in a list called labels_lc\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QyPnVtM8kQCY"},"source":["## labels without html\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ebYp-dzikQCY"},"source":["## pandas dataframe"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vLrXVGABkQCY"},"source":["def export2csv(a_list, filename):\n","    '''\n","    provide list name first\n","    provide filename as a string\n","    '''\n","    df = pd.DataFrame(a_list)\n","    df.to_csv(filename, encoding='utf-8', index=False)\n","    print(f\"{filename} is in your project folder!\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e8kGuG86kQCZ"},"source":["## export to csv\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PXOsj1rhkQCZ"},"source":["# Reality - not handed to you so cleanly\n","\n","### Let's scrape the disorganized section"]},{"cell_type":"code","metadata":{"id":"ON8edAP4kQCZ"},"source":["## soup captures the entire page.\n","## we narrow it down to capture our target section and store in object called dis\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LfQIz-BHkQCZ"},"source":["### Scrape the non-tabular data in id=\"disorganized\""]},{"cell_type":"code","metadata":{"id":"U6CkJHJfkQCZ"},"source":["## find the labels (column headers)\n","## using list comprehension save in a list called labels_lc\n","labels_lc = [label.get_text().strip(':') for label in labels]\n","labels_lc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Keuy894kQCa"},"source":["## find class ceo and store in object called ceos\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ztFbtT1kQCa"},"source":["##THIS ONE REQUIRES US TO TYPE THE COLUMN HEADERS AS KEYS\n","## NEXT CELL IS MORE EFFICIENT\n","\n","## we only printed them out. let's store in a list of dicts called ceo_dict_list\n","ceo_dict_list = []\n","for ceo in ceos:\n","    each_ceo = ceo.find_all(\"dt\")\n","    rank = each_ceo[0].get_text()\n","    name = each_ceo[1].get_text()\n","    annual_compensation = each_ceo[2].get_text()\n","    company = each_ceo[3].get_text()\n","    ceo_dict = {\"Rank\": rank, \"Name\": name, \"Company\": company, \"Annual Compensation\": annual_compensation}\n","    ceo_dict_list.append(ceo_dict)\n","ceo_dict_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j3Et7aYokQCa"},"source":["## we only printed them out. let's store in a list of dicts called ceo_dict_list\n","ceo_dict_list2 = []\n","for ceo in ceos:\n","    my_list = []\n","    each_ceo = ceo.find_all(\"dt\")\n","    rank1 = each_ceo[0].get_text()\n","    name1 = each_ceo[1].get_text()\n","    annual_compensation1 = each_ceo[2].get_text()\n","    company1 = each_ceo[3].get_text()\n","    my_list.extend([rank1, name1, annual_compensation1, company1])\n","    print(my_list)\n","    ceo_dict = dict(zip(labels_lc, my_list))\n","    ceo_dict_list2.append(ceo_dict)\n","ceo_dict_list2\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TTxmq3ZJkQCa"},"source":["## use pandas to write to csv file\n","filename = \"disorganized.csv\" ## what are file name is\n","df = pd.DataFrame(ceo_dict_list2) ## we turn our list of dicts into a dataframe which we're call df\n","df.to_csv(filename, encoding='utf-8', index=False) ## export to csv as utf-8 coding (it just has to be this)\n","\n","print(f\"{filename} is in your project folder!\") ## a print out that tells us the file is ready"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4rAOWI7mkQCb"},"source":[""],"execution_count":null,"outputs":[]}]}